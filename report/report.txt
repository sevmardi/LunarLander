
## Abstract
-----------
In this paper our aim is to solve the self-taught lunar lander task with reinforcement learning agent with no prior knowledge of control. The goal for the agent is to learn the dynamics of the lunar lander and perform a soft landing on a designated landing zone. The agent's learning process is completelty automated and unsupervised, merely based on observable reward from its surroudings (environment). We deomposed the task into two parts: the stable controle task and the direct landing task and created simulated environments for each task. We then worked and imporvised with Policy Gradient algorithm to train our agents. Our experiments showed it is possible to solve the self-taught lunar lander task with this approach. Furthermore, we want to invesitage more general approaches to solve self-taught control policies in the future. 


## Introduction
--------------
The orginal lunar lander task requires an agent to perform soft landigs on designated landing zones with controlable thrust and rotation. It was first introduced in 1979 in a game developed by Atari, Inc. and became a wide spread game concept. Recently the same idea and concept was used on real world rockets such as Falcon 9 rocket by SpaceX, which we somehow drew inspiration and elicited us to invistage further. Instead of using traditioal approache, we wanted to study whether there were methods other than explicitly programmed control algorithms. 

One of methods and core idea of this paper is using reinforcement learning, which is in its trun capable of learning through unknown dynamics of an environment. In the reinfrocement learning perspective, the lunar lander task has a continuos state space and the agent must learn accurate control policices to perfrom a successful landing. Most conventional reinforcement learning algorithms need to either iterate over states and store expceted reward values for actions at each state or utilize linear regression or similar algorithms to approximate these values. These characterticts kept them from learning effectly from high dimensional and continiuos inputs and complex policies[1]. 

However, recent breakthroughts in deep learning, which strengthenned the capability of neural networks, has contributed significantly to reinforcement learning applications. Neural networks serves as a regression model for learning expected reward values as well as method of dimensionality reduction for observed features of states[2]. We will try this option and clarify more about this in the expermints section.


## Related Work
---------------
There are several reinforcement learning related approahces to the lunar lander problem. Genetic algorithms is one of the way to achieve a simpliefed version of the task[3]. Another more complex method is guided policy serach, which is successful in learning mechanics of robots[4]. However, no pas works using reinforcement learning can solve this task thoroughly and effectivly. 



## Problem Description
-----------------------



## Background
-------------


## Approach
------------


## Experiments
--------------


## Conclusions
---------------


## Future work
--------------



## Ref
------

[1] Zhangbo Liu.: A Guided Genetic Algorithm for the Planning in Lunar Lander Games
[2] Barry David Nichols.: Reinforcement learning in continuous state-and action-space
[3] H. Hans.: Reinforcement Learning in Continuous State Spaces 
[4] R. Shariff, T. Dick.: Lunar Lander: A Continuous-Action Case Study for Policy-Gradient Actor-Critic Algorithms