
## Abstract
-----------
In this paper our aim is to solve the self-taught lunar lander task with reinforcement learning agent with no prior knowledge of control. The goal for the agent is to learn the dynamics of the lunar lander and perform a soft landing on a designated landing zone. The agent's learning process is completely automated and unsupervised, merely based on observable reward from its surroundings (environment). We decomposed the task into two parts: the stable control task and the direct landing task and created simulated environments for each task. We then worked and improvised with Policy Gradient algorithm to train our agents. Our experiments showed it is possible to solve the self-taught lunar lander task with this approach. Furthermore, we want to investigate more general approaches to solve self-taught control policies in the future. 


## Introduction
--------------
There are two main strategies for solving reinforcement
learning problems. The first is to search in the space
of behaviors in order to find one that performs well in
the environment. The second is to use statistical tech-
niques and dynamic programming methods to estimate
the utility of taking actions in states of the world (Kael-
bling et al. 1996). Genetic algorithms (GA) and Tempo-
ral Difference (TD-based) algorithms (e.g. Q-Learning)
belong to each of the two categories, respectively.\\

\noindent Both GA and TD-based algorithms have advantages and
disadvantages. GA leads to very good exploration with
its large population that can be generated within a gen-
eration but weak exploitation with elitism selection op-
erator, because its other two operators, the crossover
and mutation operators are usually randomly working.
TD-based algorithms use two strategies to solve prob-
lems with continuous space which are discretization and
function approximation. It usually faces the curse of di-
mensionality when using discretization. With function
approximation it is said to be able to alleviate such a problem but might be stuck into certain local optima.\\




## Related Work
---------------
There are several reinforcement learning related approaches to the lunar lander problem. Genetic algorithms is one of the way to achieve a simplified version of the task[3]. Another more complex method is guided policy search, which is successful in learning mechanics of robots[4]. However, no pas works using reinforcement learning can solve this task thoroughly and effectively.


## Problem Description
-----------------------
The aim of the project is to build a learning agent to navigate a space vehicle from a starting point in space to the landing pod without crushing. The environment is called LunarLander-v2 which is provided by OpenAI's gym python package. At each step, the agent is provided with the current state of the space vehicle s which is a 8 dimensional vector of reals \(R^8\) and the agent is allowed to make one of the the four possible actions {do nothing, fire left orientation engine, fire main engine, fire right orientation engine} in one step. On making an action \(a\), the the agent gets a reward \(r\) and also get to know the new state of the vehicle \(s'\). If the vehicle moves from the top of the screen to reach landing pad at zero speed, it gets reward in the range \([100, 140]\). The vehicle is permitted to land outside the landing pad, but is penalized for such cases. The episode finishes if the vehicle crashes(gets additional reward of -100) or comes to rest(gets additional reward of +100). There is no restriction on the amount of fuel used. The agent is supposed to learn a policy \(\pi(s)\) that decides what action it must make at a given current state \(s\) by going over the past experiences defined by the tuple \(< s, a, r, s 0 >\).\\  

\noindent The objective of this project is two fold - Create an agent such that it 1) maximize the expected total reward per episode with minimum variance in test trials.  And 2) minimize the expected episode length in test trials.
The task at hand is a typical reinforcement learning problem but with a discrete state space. Q-learning is popular model-free strategy to solve a reinforcement learning problem with discrete state and action spaces. The project uses a skeleton version of recent Q-learning  algorithm called Deep Q Network learning (DQN). which uses a neural network to approximate the state-value function \(Q ∗ (s, a)\) where \(s\) is the current state and \(a\) is the action performed.\\  



## Background
-------------
The Open AI Gym offers tool-kits to test different reinforcement learning
algorithms. One of these tool-kits is LunarLander-v2 which is a game where the goal is to land a lunar module safely without crashing between a set of two goal posts. The tool-kit offers rewards for landing safely, penalizes the use of fuel and offers small rewards when the lunar lander gets closer to the target landing spot. It includes state space of 8 dimensions and an action space of 4 dimensions. The state space includes the position of the lander, its horizontal and vertical velocity as well as it’s angle, angular velocity and a binary value that represents if the feet have landed on the ground.

\noindent The original lunar lander task requires an agent to perform soft landings on designated landing zones with controllable thrust and rotation. It was first introduced in 1979 in a game developed by Atari, Inc. and became a wide spread game concept. Recently the same idea and concept was used on real world rockets such as Falcon 9 rocket by SpaceX, which we somehow drew inspiration and elicited us to investigate further. Instead of using traditional approach, we wanted to study whether there were methods other than explicitly programmed control algorithms.\\ 

\noindent The first attempt at solving the learning problem consisted of a Q-learner which used parameter
approximation by using series of neural networks which had the state space as the input and the probabilities of the actions to take as the output. This implement was based off a paper title “Playing Atari with Deep Reinforcement Learning”[1] in which an algorithm was presented that had a convolutional neural network was trained with a variant of Q-learning. While this approach proved to find a solution, we found that it took over 4 hours to converge on my machine. And sometimes even doesn't converge and keeps crushing the lander. This compelled us to find a faster solution with much quicker converge time.\\ 

\noindent One of methods and core idea of this paper is using reinforcement learning, which is in its turn capable of learning through unknown dynamics of an environment. In the reinforcement learning perspective, the lunar lander task has a continuous state space and the agent must learn accurate control policies to perform a successful landing. Most conventional reinforcement learning algorithms need to either iterate over states and store expected reward values for actions at each state or utilize linear regression or similar algorithms to approximate these values. These characteristics kept them from learning effectively from high dimensional and continuous inputs and complex policies[1].\\ 

\noindent However, recent breakthroughs in deep learning, which strengthened the capability of neural networks, has contributed significantly to reinforcement learning applications. Neural networks serves as a regression model for learning expected reward values as well as method of dimensionality reduction for observed features of states[2]. We will try this option and clarify more about this in the experiments section.\\


## Approach
------------


## Experiments
--------------


## Conclusions
---------------


## Future work
--------------



## Ref
------

[1] Zhangbo Liu.: A Guided Genetic Algorithm for the Planning in Lunar Lander Games
[2] Barry David Nichols.: Reinforcement learning in continuous state-and action-space
[3] H. Hans.: Reinforcement Learning in Continuous State Spaces 
[4] R. Shariff, T. Dick.: Lunar Lander: A Continuous-Action Case Study for Policy-Gradient Actor-Critic Algorithms
[5] https://gym.openai.com