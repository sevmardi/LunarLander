
## Abstract
-----------
In this paper our aim is to solve the self-taught lunar lander task with reinforcement learning agent with no prior knowledge of control. The goal for the agent is to learn the dynamics of the lunar lander and perform a soft landing on a designated landing zone. The agent's learning process is completely automated and unsupervised, merely based on observable reward from its surroundings (environment). We decomposed the task into two parts: the stable control task and the direct landing task and created simulated environments for each task. We then worked and improvised with Policy Gradient algorithm to train our agents. Our experiments showed it is possible to solve the self-taught lunar lander task with this approach. Furthermore, we want to investigate more general approaches to solve self-taught control policies in the future. 


## Introduction
--------------
The original lunar lander task requires an agent to perform soft landings on designated landing zones with controllable thrust and rotation. It was first introduced in 1979 in a game developed by Atari, Inc. and became a wide spread game concept. Recently the same idea and concept was used on real world rockets such as Falcon 9 rocket by SpaceX, which we somehow drew inspiration and elicited us to investigate further. Instead of using traditional approach, we wanted to study whether there were methods other than explicitly programmed control algorithms. 

One of methods and core idea of this paper is using reinforcement learning, which is in its turn capable of learning through unknown dynamics of an environment. In the reinforcement learning perspective, the lunar lander task has a continuous state space and the agent must learn accurate control policies to perform a successful landing. Most conventional reinforcement learning algorithms need to either iterate over states and store expected reward values for actions at each state or utilize linear regression or similar algorithms to approximate these values. These characteristics kept them from learning effectively from high dimensional and continuous inputs and complex policies[1]. 

However, recent breakthroughs in deep learning, which strengthened the capability of neural networks, has contributed significantly to reinforcement learning applications. Neural networks serves as a regression model for learning expected reward values as well as method of dimensionality reduction for observed features of states[2]. We will try this option and clarify more about this in the experiments section.


## Related Work
---------------
There are several reinforcement learning related approaches to the lunar lander problem. Genetic algorithms is one of the way to achieve a simplified version of the task[3]. Another more complex method is guided policy search, which is successful in learning mechanics of robots[4]. However, no pas works using reinforcement learning can solve this task thoroughly and effectively.


## Problem Description
-----------------------
The aim of the project is to build a learning agent to navigate a space vehicle from a starting point in space to the landing pod without crushing. The environment is called LunarLander-v2 which is provided by OpenAI's gym python package. At each step, the agent is provided with the current state of the space vehicle s which is a 8 dimensional vector of reals R^8 and the agent is allowed to make one of the the four possible actions {do nothing, fire left orientation engine, fire main engine, fire right orientation engine } in one step. On making an action a, the the agent gets a reward r and also get to know the new state of the vehicle s'. If the vehicle moves from the top of the screen to reach landing pad at zero speed, it gets reward in the range [100, 140]. The vehicle is permitted to land outside the landing pad, but is penalized for such cases. The episode finishes if the vehicle crashes(gets additional
reward of -100) or comes to rest(gets additional reward of +100) or hits the maximum episode length of 1000. There is no restriction on the amount of fuel used. The agent is supposed to learn a policy π(s) that decides what action it must make at a given current state s by going over the past experiences defined by the tuple < s, a, r, s 0 >.  The objective of this project is two fold - Create an agent such that it 1) maximize the expected total reward per episode with minimum variance in test trials. The task at hand is a typical reinforcement learning problem but with a discrete state space. Q-learning is popular model-free strategy to solve a reinforcement learning problem with discrete state and action spaces. The project uses a recent version of Q-learning called Deep Q Network learning (DQN). which uses a neural network to approximate the state-value function Q ∗ (s, a) where s is the current state and a is the action performed.  



## Background
-------------
The Open AI Gym offers tool-kits to test different reinforcement learning
algorithms. One of these tool-kits is LunarLander-v2 which is a game where the goal is to land a lunar module safely without crashing between a set of two goal posts. The tool-kit offers rewards for landing safely, penalizes the use of fuel and offers small rewards when the lunar lander gets closer to the target landing spot. It includes state space of 8 dimensions and an action space of 4 dimensions. The state space includes the position of the lander, its horizontal and vertical velocity as well as it’s angle, angular velocity and a binary value that represents if the feet have landed on the ground.

The first attempt at solving the learning problem consisted of a Q-learner which used parameter
approximation by using series of neural nets which had the state space as the input and the probabilities of the actions to take as the output. This implement was based off a paper title “Playing Atari with Deep Reinforcement Learning”[1] in which an algorithm was presented that had a convolutional neural network was trained with a variant of Q-learning. While this approach proved to find a solution, we found that it took over 4 hours to converge on my machine. And sometimes even doesn't converge and keeps crushing the lander. This compelled us to find a faster solution with much quicker converge time. 


## Approach
------------


## Experiments
--------------


## Conclusions
---------------


## Future work
--------------



## Ref
------

[1] Zhangbo Liu.: A Guided Genetic Algorithm for the Planning in Lunar Lander Games
[2] Barry David Nichols.: Reinforcement learning in continuous state-and action-space
[3] H. Hans.: Reinforcement Learning in Continuous State Spaces 
[4] R. Shariff, T. Dick.: Lunar Lander: A Continuous-Action Case Study for Policy-Gradient Actor-Critic Algorithms
[5] https://gym.openai.com